# **ETL Pipeline with Apache Airflow, Docker & DVC**  

ğŸš€ A **scalable, automated ETL pipeline** using **Apache Airflow, Docker, and DVC** for data versioning and orchestration. This project extracts **news articles from BBC** (`https://www.bbc.com`), processes the data, and stores it in a **data warehouse**â€”ensuring **reproducibility, scalability, and monitoring**.  

## ğŸ“Œ **Features**  
âœ… **Containerized Deployment** â€“ Uses Docker & Docker Compose for easy setup & scalability  
âœ… **Data Versioning** â€“ Tracks datasets with **DVC** and stores them in **Google Drive**  
âœ… **Web Scraping** â€“ Extracts news articles from **BBC** (`https://www.bbc.com`)  
âœ… **Extract** â€“ Pulls data from BBC using **BeautifulSoup / Scrapy**  
âœ… **Transform** â€“ Cleans, tokenizes, and extracts key information  
âœ… **Load** â€“ Stores processed data in a warehouse (PostgreSQL, BigQuery, Snowflake)  
âœ… **Workflow Orchestration** â€“ Uses Apache Airflow DAGs to manage dependencies  
âœ… **Monitoring & Logging** â€“ Tracks execution, logs errors, and enables alerts  

---

## ğŸ—ï¸ **Tech Stack**  
- **Apache Airflow** ğŸŒ€ (Orchestration)  
- **Docker & Docker Compose** ğŸ³ (Containerization)  
- **DVC (Data Version Control)** ğŸ”„ (Dataset Management)  
- **Google Drive** â˜ï¸ (Remote Storage)  
- **BeautifulSoup / Scrapy** ğŸ“° (Web Scraping)  
- **Python** ğŸ (Transformation Logic)  

---

## ğŸš€ **Getting Started**  

### **1ï¸âƒ£ Clone the Repository**  
```bash
https://github.com/usman7384/automated-ETL-airflow.git
cd automated-ETL-airflow
```

### **2ï¸âƒ£ Set Up Environment Variables**  
Rename `.env.example` to `.env` and update configurations if needed.

```bash
cp .env.example .env
```

### **3ï¸âƒ£ Set Up DVC with Google Drive**  
First, install **DVC** if you havenâ€™t already:  
```bash
pip install dvc
```

#### **Initialize DVC**  
```bash
dvc init
```

#### **Configure Google Drive as Remote Storage**  
```bash
dvc remote add -d gdrive_remote gdrive://your-drive-folder-id
dvc remote modify gdrive_remote gdrive_use_service_account true
```
To authenticate with Google Drive, follow the [DVC Google Drive Guide](https://dvc.org/doc/user-guide/setup-google-drive-remote).  

#### **Pull Data from Remote Storage**  
```bash
dvc pull
```

#### **Track New Data Files**  
```bash
dvc add data/bbc_articles.csv
git add data/bbc_articles.csv.dvc .gitignore
git commit -m "Added BBC scraped articles"
git push origin main
dvc push
```

---

### **4ï¸âƒ£ Start Airflow Services**  
Run the following command to start Apache Airflow with Docker Compose:  
```bash
docker-compose up -d
```
This will start all necessary containers, including the **Airflow scheduler, web server, and PostgreSQL database**.

### **5ï¸âƒ£ Access the Airflow UI**  
Once the services are running, open your browser and go to:  
ğŸ“Œ **Airflow UI:** [`http://localhost:8080`](http://localhost:8080)  
ğŸ“Œ **Username:** `airflow` | **Password:** `airflow` (default)  

### **6ï¸âƒ£ Deploy Your DAGs**  
Place your DAG files inside the `dags/` folder. Airflow will automatically detect and execute them based on the defined schedule.

### **7ï¸âƒ£ Monitor Logs & Status**  
Check logs for debugging:  
```bash
docker-compose logs -f
```
List running containers:  
```bash
docker ps
```

### **8ï¸âƒ£ Stop Services**  
To stop all running containers:  
```bash
docker-compose down
```
